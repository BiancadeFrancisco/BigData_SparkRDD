# BigData_SparkRDD

## Descrição Projeto

**Conjunto de dados:**
Conjunto de dados com palavras aleatórias.

**Arquivo:** .txt

**Objetivo:**
Através do PySpark, explorar um conjunto de dados (palavras aleatórias) e manipulá-las conforme necessidade de análise.

**Observação:**
O PySpark é uma ferramenta de processamento de dados em larga escala que se baseia na linguagem de programação Python e no framework Spark.
O RDD (Resilient Distributed Dataset) é uma abstração principal que o Apache Spark oferece. É um conjunto de dados distribuído resiliente, que é uma coleção de elementos particionados nos nós do cluster que podem ser operados em paralelo 

**IDE:** Colab

**Linguagem:** Python 

**Principais bibliotecas utilizadas:**
•	PySpark, para explorar e analisar big data.

**Resultado:**
Foi possível filtrar o banco de dados, buscando palavras chaves necessárias e contando quantas vezes a mesma aparece dentro do dataset.
